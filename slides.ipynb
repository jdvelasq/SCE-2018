{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BIG DATA Y COMPUTACIÓN DE ALTO DESEMPEÑO: UNA INTRODUCCIÓN PARA EL ESTADÍSTICO\n",
    "\n",
    "\n",
    "\n",
    "Haga click [aquí](http://nbviewer.jupyter.org/format/slides/github/jdvelasq/big-data-y-comp-alto-desempeno/blob/master/Slides.ipynb?flush_cache=true#/) para ver esta presentación en nbviewer.\n",
    "\n",
    "<img src=\"img/firma-social-media.jpg\" alt=\"\" width=\"500\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/evo-big-data.jpg\" alt=\"\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/servicios-web.jpg\" alt=\"\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Hadoop & Map/Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/map-reduce.jpg\" alt=\"\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/mr-word-count.jpg\" alt=\"\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalidades\n",
    "\n",
    "## Modos de ejecución\n",
    "\n",
    "\n",
    "* Modo local (un solo hilo de ejecución en la máquina local, para aprendizaje y desarrollo). El sistema local de archivos funciona como el HDFS.\n",
    "\n",
    "\n",
    "* `=====>`  Modo seudo-distribuido (múltiples hilos en la máquina local, para desarrollo). El HDFS es alcanzable como un servicio.\n",
    "\n",
    "\n",
    "* Cluster (ambiente productivo)\n",
    "\n",
    "\n",
    "\n",
    "## Desarrollo de aplicaciones\n",
    "\n",
    "\n",
    "* Programación directa en Java (Hadoop)\n",
    "\n",
    "\n",
    "* `=====>` Programación usando un lenguaje interpretado (Hadoop-streaming)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sistemas operativos\n",
    "\n",
    "* Microsoft Windows (dificil).\n",
    "\n",
    "\n",
    "* Mac OS: instalación directa. \n",
    "\n",
    "\n",
    "* Ubuntu Linux: instalación directa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo del conteo de palabras\n",
    "\n",
    "Se usará Hadoop-streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "A B C A\n",
    "A D D A\n",
    "A K M C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "B A C Y B\n",
    "U O Y Y A\n",
    "A B I T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "A C D A\n",
    "A K B \n",
    "A N H I D A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## mapper.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.R\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.R\n",
    "#! /usr/bin/env Rscript\n",
    "\n",
    "input <- file('stdin', 'r')\n",
    "while(TRUE) {\n",
    "    row <- readLines(input, n=1)\n",
    "    if( length(row) == 0 ){\n",
    "        break\n",
    "    }\n",
    "    words <- strsplit(row, \" \")[[1]]\n",
    "    for(word in words){\n",
    "        if(word != '')\n",
    "            write(cat(word,'\\t1',sep=''), \"\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\t1\r\n",
      "B\t1\r\n",
      "C\t1\r\n",
      "A\t1\r\n",
      "A\t1\r\n",
      "D\t1\r\n",
      "D\t1\r\n",
      "A\t1\r\n",
      "A\t1\r\n",
      "K\t1\r\n",
      "Error in cat(x, file = file, sep = c(rep.int(sep, ncolumns - 1), \"\\n\"),  : \r\n",
      "  ignoring SIGPIPE signal\r\n",
      "Calls: write -> cat\r\n",
      "Execution halted\r\n"
     ]
    }
   ],
   "source": [
    "## El programa anterior se hace ejecutable\n",
    "!chmod +x mapper.R\n",
    "\n",
    "## Verificación\n",
    "!cat ./input/text*.txt | ./mapper.R | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## reducer.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.R\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.R\n",
    "#!/usr/bin/env Rscript\n",
    "\n",
    "curkey <- NULL\n",
    "total <- 0\n",
    "input <- file('stdin', 'r')\n",
    "while(TRUE) {\n",
    "    row <- readLines(input, n=1)\n",
    "    if( length(row) == 0 ){\n",
    "        break\n",
    "    }\n",
    "    x <- strsplit(row, \"\\t\")[[1]]\n",
    "    key <- x[1]\n",
    "    value <- strtoi(x[2])\n",
    "    if(!is.null(curkey) && key == curkey){\n",
    "        total <- total + value\n",
    "    }\n",
    "    else{\n",
    "        if( !is.null(curkey) ) {\n",
    "            write(cat(curkey,'\\t', total), \"\")\n",
    "        }\n",
    "        curkey <- key\n",
    "        total <- value\n",
    "    }\n",
    "}\n",
    "write(cat(curkey,'\\t', total), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning message:\r\n",
      "In readLines(input, n = 1) : incomplete final line found on 'stdin'\r\n",
      "A \t 13\r\n",
      "B \t 5\r\n",
      "C \t 4\r\n",
      "D \t 4\r\n",
      "H \t 1\r\n",
      "I \t 2\r\n",
      "K \t 2\r\n",
      "M \t 1\r\n",
      "N \t 1\r\n",
      "O \t 1\r\n",
      "T \t 1\r\n",
      "U \t 1\r\n",
      "Y \t 3\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x reducer.R\n",
    "!cat ./input/text*.txt | ./mapper.R | sort | ./reducer.R "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejecución en Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 jdvelasq supergroup         25 2018-11-08 23:36 /user/jdvelasq/input/text0.txt\r\n",
      "-rw-r--r--   1 jdvelasq supergroup         29 2018-11-08 23:36 /user/jdvelasq/input/text1.txt\r\n",
      "-rw-r--r--   1 jdvelasq supergroup         28 2018-11-08 23:36 /user/jdvelasq/input/text2.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir /user\n",
    "!hadoop fs -mkdir /user/jdvelasq\n",
    "!hadoop fs -mkdir /user/jdvelasq/input\n",
    "!hadoop fs -copyFromLocal  input/* /user/jdvelasq/input\n",
    "!hadoop fs -ls /user/jdvelasq/input/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-input input -output output  -mapper mapper.R -reducer reducer.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 jdvelasq supergroup          0 2018-11-08 23:37 /user/jdvelasq/output/_SUCCESS\r\n",
      "-rw-r--r--   1 jdvelasq supergroup         79 2018-11-08 23:37 /user/jdvelasq/output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/jdvelasq/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A \t 13\r\n",
      "B \t 5\r\n",
      "C \t 4\r\n",
      "D \t 4\r\n",
      "H \t 1\r\n",
      "I \t 2\r\n",
      "K \t 2\r\n",
      "M \t 1\r\n",
      "N \t 1\r\n",
      "O \t 1\r\n",
      "T \t 1\r\n",
      "U \t 1\r\n",
      "Y \t 3\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /user/jdvelasq/output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "http://localhost:50070/explorer.html#/user/jdvelasq/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problemas Típicos que se puden programar usando Map/Reduce\n",
    "\n",
    "* Valores máximo o mínimo para una clave.\n",
    "\n",
    "\n",
    "* Ordenamiento del archivo.\n",
    "\n",
    "\n",
    "* Sumas y promedios por claves.\n",
    "\n",
    "\n",
    "* Obtener los N registros más ...\n",
    "\n",
    "\n",
    "* Asociar por claves:\n",
    "\n",
    "\n",
    "    0   A, B                  A  0, 1\n",
    "    1   A, B       ======>    B  0, 1, 2  \n",
    "    2   B, C                  C  2\n",
    "    \n",
    "    \n",
    "**La mayor parte de problemas no pueden solucionarse usando un proceso simple Map/Reduce**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/mr-jobs.jpg\" alt=\"\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WordCount en Pig\n",
    "\n",
    "Se interactua a través de la líneas de comandos. El grupo de investigación escribió un magic que permite interactuar con Apache Pig desde Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext bigdata\n",
    "%pig_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%pig\n",
    "lines = LOAD 'input/text*.txt' AS (line:CHARARRAY);\n",
    "\n",
    "-- genera una tabla llamada words con una palabra por registro\n",
    "words = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) AS word;\n",
    "\n",
    "-- agrupa los registros que tienen la misma palabra\n",
    "grouped = GROUP words BY word;\n",
    "\n",
    "-- genera una variable que cuenta las ocurrencias por cada grupo\n",
    "wordcount = FOREACH grouped GENERATE group, COUNT(words);\n",
    "\n",
    "-- selecciona las primeras 15 palabras\n",
    "s = LIMIT wordcount 15;\n",
    "\n",
    "-- imprime en pantalla las primeras 15 palabras\n",
    "STORE s INTO 'output-pig';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 jdvelasq supergroup          0 2018-11-09 07:22 output-pig/_SUCCESS\r\n",
      "-rw-r--r--   1 jdvelasq supergroup         53 2018-11-09 07:22 output-pig/part-r-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls output-pig/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\t13\r\n",
      "B\t5\r\n",
      "C\t4\r\n",
      "D\t4\r\n",
      "H\t1\r\n",
      "I\t2\r\n",
      "K\t2\r\n",
      "M\t1\r\n",
      "N\t1\r\n",
      "O\t1\r\n",
      "T\t1\r\n",
      "U\t1\r\n",
      "Y\t3\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /user/jdvelasq/output-pig/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/hdfs-pig-1.jpg\" alt=\"\" width=\"900\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/hdfs-pig-2.jpg\" alt=\"\" width=\"900\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/hdfs-pig-3.jpg\" alt=\"\" width=\"900\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ejemplos de Apache Pig\n",
    "\n",
    "\n",
    "\n",
    "http://nbviewer.jupyter.org/github/jdvelasq/AGD-03-Pig/blob/master/02-basics.ipynb\n",
    "    \n",
    "    \n",
    "http://nbviewer.jupyter.org/github/jdvelasq/AGD-03-Pig/blob/master/04-tipos-de-datos.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hive initialized!\n"
     ]
    }
   ],
   "source": [
    "%hive_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t7\n",
      "A\t13\n",
      "B\t5\n",
      "C\t4\n",
      "D\t4\n",
      "H\t1\n",
      "I\t2\n",
      "K\t2\n",
      "M\t1\n",
      "N\t1\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS docs;\n",
    "DROP TABLE IF EXISTS word_counts;\n",
    "\n",
    "CREATE TABLE docs (line STRING);\n",
    "\n",
    "LOAD DATA LOCAL INPATH \n",
    "    'input/text*.txt' \n",
    "OVERWRITE INTO TABLE docs;\n",
    "\n",
    "CREATE TABLE word_counts \n",
    "AS\n",
    "    SELECT word, count(1) AS count \n",
    "    FROM\n",
    "        (SELECT explode(split(line, '\\\\s')) AS word FROM docs) w\n",
    "GROUP BY \n",
    "    word\n",
    "ORDER BY \n",
    "    word;\n",
    "\n",
    "SELECT * FROM word_counts LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Características de Apache Hive\n",
    "\n",
    "\n",
    "* El lenguaje tiene muchas similitudes con SQL y resulta fácil \n",
    "\n",
    "\n",
    "* Los archivos se guardan directamente en disco duro como texto.\n",
    "\n",
    "\n",
    "* A continuación se presentan los principales puntos de diferencia con otros sistemas de bases de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tbl0.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile tbl0.csv\n",
    "1,D,4,2016-06-25,a:e:c:d,bb#10:dd#20:cc#40\n",
    "2,C,4,2015-05-14,a:c,dd#40:bb#20:cc#10\n",
    "3,D,6,2014-12-26,b:d,aa#10:bb#40\n",
    "4,D,5,2016-06-25,a:c:e:b:d,bb#40:dd#20:aa#10:cc#30\n",
    "5,C,7,2016-05-23,d:e:c,cc#10:aa#20\n",
    "6,A,2,2018-06-14,a:d,aa#20\n",
    "7,B,3,2014-12-22,a:e:d,cc#20\n",
    "8,C,6,2015-08-20,d:a:c:e,cc#10:aa#20\n",
    "9,B,3,2017-12-08,b:a:c:e,cc#40:dd#10:aa#30:bb#20\n",
    "10,B,7,2015-07-28,c:d:e:a:b,aa#10:dd#40:cc#30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tD\t4\t2016-06-25\t[\"a\",\"e\",\"c\",\"d\"]\t{\"bb\":10,\"dd\":20,\"cc\":40}\n",
      "2\tC\t4\t2015-05-14\t[\"a\",\"c\"]\t{\"dd\":40,\"bb\":20,\"cc\":10}\n",
      "3\tD\t6\t2014-12-26\t[\"b\",\"d\"]\t{\"aa\":10,\"bb\":40}\n",
      "4\tD\t5\t2016-06-25\t[\"a\",\"c\",\"e\",\"b\",\"d\"]\t{\"bb\":40,\"dd\":20,\"aa\":10,\"cc\":30}\n",
      "5\tC\t7\t2016-05-23\t[\"d\",\"e\",\"c\"]\t{\"cc\":10,\"aa\":20}\n",
      "6\tA\t2\t2018-06-14\t[\"a\",\"d\"]\t{\"aa\":20}\n",
      "7\tB\t3\t2014-12-22\t[\"a\",\"e\",\"d\"]\t{\"cc\":20}\n",
      "8\tC\t6\t2015-08-20\t[\"d\",\"a\",\"c\",\"e\"]\t{\"cc\":10,\"aa\":20}\n",
      "9\tB\t3\t2017-12-08\t[\"b\",\"a\",\"c\",\"e\"]\t{\"cc\":40,\"dd\":10,\"aa\":30,\"bb\":20}\n",
      "10\tB\t7\t2015-07-28\t[\"c\",\"d\",\"e\",\"a\",\"b\"]\t{\"aa\":10,\"dd\":40,\"cc\":30}\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS tbl0;\n",
    "CREATE TABLE tbl0 (\n",
    "    c1 INT,\n",
    "    c2 STRING,\n",
    "    c3 INT,\n",
    "    c4 DATE,\n",
    "    c5 ARRAY<CHAR(1)>, \n",
    "    c6 MAP<STRING, INT>\n",
    ")\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ','\n",
    "COLLECTION ITEMS TERMINATED BY ':'\n",
    "MAP KEYS TERMINATED BY '#'\n",
    "LINES TERMINATED BY '\\n';\n",
    "\n",
    "LOAD DATA LOCAL INPATH 'tbl0.csv' INTO TABLE tbl0;\n",
    "\n",
    "SELECT * FROM tbl0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/hdfs-hive-1.jpg\" alt=\"\" width=\"900\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from operator import add\n",
    "\n",
    "APP_NAME = \"My First Spark Application\"\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def main(sc): \n",
    "    text = sc.textFile('input/text*.txt')\n",
    "    words = text.flatMap(tokenize)\n",
    "    wc = words.map(lambda x: (x,1))\n",
    "    counts = wc.reduceByKey(add)\n",
    "    counts.saveAsTextFile(\"output-spark\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conf = SparkConf().setAppName(APP_NAME) \n",
    "    conf = conf.setMaster(\"local[*]\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    main(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   1 jdvelasq supergroup          0 2018-11-09 08:39 output-spark/_SUCCESS\r\n",
      "-rw-r--r--   1 jdvelasq supergroup         45 2018-11-09 08:39 output-spark/part-00000\r\n",
      "-rw-r--r--   1 jdvelasq supergroup         37 2018-11-09 08:39 output-spark/part-00001\r\n",
      "-rw-r--r--   1 jdvelasq supergroup         36 2018-11-09 08:39 output-spark/part-00002\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls output-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('B', 5)\r\n",
      "('C', 4)\r\n",
      "('D', 4)\r\n",
      "('T', 1)\r\n",
      "('N', 1)\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /user/jdvelasq/output-spark/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 7, 7, 1, 9, 9, 7, 1, 7, 6, 11, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## en este ejemplo se pasa una función arbitraria a `map`\n",
    "from operator import add\n",
    "rdd = sc.textFile('input/text*.txt')\n",
    "rdd = rdd.map(len)\n",
    "print(rdd.collect())\n",
    "rdd = rdd.reduce(add)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    _C5\n",
    "FROM \n",
    "    csv.`datos.csv`\n",
    "ORDER BY\n",
    "    _c5\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).write.save('temp', format=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - jdvelasq supergroup          0 2018-11-08 23:36 /user/jdvelasq/input\r\n",
      "drwxr-xr-x   - jdvelasq supergroup          0 2018-11-08 23:37 /user/jdvelasq/output\r\n",
      "drwxr-xr-x   - jdvelasq supergroup          0 2018-11-09 07:22 /user/jdvelasq/output-pig\r\n",
      "drwxr-xr-x   - jdvelasq supergroup          0 2018-11-09 08:39 /user/jdvelasq/output-spark\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/jdvelasq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/jdvelasq/output-pig/*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm /user/jdvelasq/output-pig/*\n",
    "!hadoop fs -rmdir /user/jdvelasq/output-pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir /user/jdvelasq/output-pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/big-data-analytics.jpg\" alt=\"\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css():\n",
    "    style = open(\"custom.css\", \"r\").read()\n",
    "    return HTML(style)\n",
    "css()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
